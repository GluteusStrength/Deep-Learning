{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차적으로 git clone을 진행한다.\n",
    "# ultralytics yolov5를 활용.\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%pip install -qr requriements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c77fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 module upload\n",
    "import torch\n",
    "# yaml 파일을 만들기 위한 module\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# json file\n",
    "import json\n",
    "import os\n",
    "# XML 어노테이션 파일\n",
    "import xml.etree.ElementTree as ET\n",
    "# 이미지를 한 번에 긁어모으기 위한 glob 모듈\n",
    "from glob import glob\n",
    "# 파일 이동\n",
    "import shutil\n",
    "# Image를 open\n",
    "from PIL import Image\n",
    "# plt.imshow()후 커널 dead 현상 제거\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c6bc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda 연산\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb59cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Desktop\\recognition_project\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb22617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b092b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 file을 train과 valid, test로 나눠준다\n",
    "# directory 생성 후 나중에 상황에 맞게 분리 해준다\n",
    "# path = \"./archive\"\n",
    "# img_path = path+\"\\\\\"+\"images\"\n",
    "# annot_path = path+\"\\\\\"+\"annotations\"\n",
    "# os.mkdir(img_path+\"\\\\\"+\"train\")\n",
    "# os.mkdir(img_path+\"\\\\\"+\"valid\")\n",
    "# os.mkdir(img_path+\"\\\\\"+\"test\")\n",
    "# os.mkdir(annot_path+\"\\\\\"+\"train\")\n",
    "# os.mkdir(annot_path+\"\\\\\"+\"valid\")\n",
    "# os.mkdir(annot_path+\"\\\\\"+\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb4206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yolov5 model load\n",
    "# 우선은 따로 구축하진 않고 있는 parameter 활용\n",
    "# parameter 활용 시 customizing 하는 방법 생각\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2e6a5",
   "metadata": {},
   "source": [
    "우선 kaggle의 데이터셋에는 annotation file까지 준비가 된 상태이다.\n",
    "1. 데이터 파일을 coco 형식으로 만들어 주거나 또는 이미 xml로 annotation 되어 있다.\n",
    "    - 우선 이미지와 객체에 대한 label 정보를 포함하는 JSON 파일을 생성한다\n",
    "    - 즉, 이미지와 객체에 대한 label 정보를 포함하는 JSON 파일을 이용하여 이미지와 라벨을 저장한다.\n",
    "2. 'data.yaml' 파일 생성\n",
    "    - 클래스 개수와 클래스 이름 설정\n",
    "        - coco 형식으로 구성된 데이터셋에 대한 클래스 개수와 클래스 이름을 설정한다. 이 정보는 data.yaml에 포함된다\n",
    "    - 학습/검증 데이터 경로 설정\n",
    "        - 학습과 검증 데이터 경로를 설정. 이 경로 정보는 data.yaml 파일에 포함\n",
    "    - data.yaml 파일 생성\n",
    "        - 설정된 정보를 이용해 data.yaml 파일을 생성. 생성된 파일은 coco 데이터셋을 이용하는 YOLOv5 모델에서 학습에 사용된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml 파일의 구성이 어떻게 돼 있는지 확인\n",
    "xml_file = \"./archive/annotations/train/hard_hat_workers1.xml\"\n",
    "# xml 파일 열기(parsing)\n",
    "tree = ET.parse(xml_file)\n",
    "# xml 파일의 root 요소 가져오기\n",
    "root = tree.getroot()\n",
    "# root 요소의 tag 이름 출력\n",
    "print(root.tag)\n",
    "# 루트 요소의 자식 요소 출력\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f83a9d",
   "metadata": {},
   "source": [
    "다음의 아래의 코드는 annotation된 것을 YOLO에 맞게 변환 시켜준 것이다.\n",
    "- COCO 형식이 아닌 PASCAL VOC형태.\n",
    "- XML 형태의 ANNOTATION 파일을 YOLO 형식에 맞게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 형식으로 변환할 class 이름을 지정\n",
    "classes = [\"helmet\", \"head\", \"person\"]\n",
    "# xml 파일과 이미지 파일이 저장된 폴더 경로를 지정\n",
    "# 각 train, test, valid 별로 진행해 준다.\n",
    "types = [\"train\", \"valid\", \"test\"]\n",
    "# yolo 형식으로 변환된 파일을 저장할 폴더를 생성\n",
    "%cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\\\\archive\"\n",
    "if not os.path.exists(\"labels\"):\n",
    "    os.makedirs(\"labels\")\n",
    "    %cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\\\\archive\\\\labels\"\n",
    "    os.makedirs(\"train\")\n",
    "    os.makedirs(\"valid\")\n",
    "    os.makedirs(\"test\")\n",
    "    %cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\\\\archive\"\n",
    "# xml 파일을 parsing해서 yolo 형식으로 변환\n",
    "# os.listdir -> xml 데이터들이 담겨 있다.\n",
    "cnt = 0\n",
    "for t in types:\n",
    "    xml_path = \"./annotations/\" + t\n",
    "    image_path = \"./images/\" + t\n",
    "    for xml_file_name in os.listdir(xml_path):\n",
    "        # ./annotations/train\\\\*.xml\n",
    "        xml_file_path = os.path.join(xml_path, xml_file_name)\n",
    "        # xml 파일을 parsing\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        # 이미지 파일 경로 가져오기\n",
    "        # image_file_name에는 해당 xml에 대응되는 이미지 파일의 이름을 나타낸다.\n",
    "        image_file_name = root.find(\"filename\").text\n",
    "        image_file_path = os.path.join(image_path, image_file_name)\n",
    "        # 이미지 파일 열기\n",
    "        img = Image.open(image_file_path)\n",
    "        img_width, img_height = img.size # 폭 x 높이\n",
    "        # YOLO 형식으로 변환 단계\n",
    "        # xml_file_name[:-4] 에는 .xml 전까지의 파일 명까지만 cut\n",
    "        # .txt형태로 변환\n",
    "        # 각 directory별로 들어가게 된다.\n",
    "        yolo_file_path = os.path.join(\"./labels\",t,xml_file_name[:-4] + \".txt\")\n",
    "        with open(yolo_file_path, \"w\") as f:\n",
    "            # 해당 xml 파일에서 <object> element를 찾아서 반복적으로 처리하는 코드.\n",
    "            # <object> element는 객체의 정보를 포함하고 있다.\n",
    "            # 각 객체의 정보를 추출한다. 추출된 정보를 사용하여 YOLO 형식으로 변환한 뒤, 텍스트 파일에 저장\n",
    "            # 반복문을 통해 object를 하나씩 가져온다.\n",
    "            for obj in root.findall(\"object\"):\n",
    "                # 각 객체 클래스의 클래스 이름을 문자열로 저장\n",
    "                # class명을 하나씩 가져온다\n",
    "                class_name = obj.find(\"name\").text\n",
    "                # 해당 cell의 맨 위에 classes를 정의하였다.\n",
    "                # classes.index(class_name) 이라고 하면 내가 정의한 list의 순서대로 index를 반환한다.\n",
    "                class_id = classes.index(class_name)\n",
    "\n",
    "                # 해당 <object> element에서 하위 element인 bndbox element를 찾아서 반환한다.\n",
    "                bbox = obj.find(\"bndbox\")\n",
    "                x_min = int(bbox.find(\"xmin\").text)\n",
    "                y_min = int(bbox.find(\"ymin\").text)\n",
    "                x_max = int(bbox.find(\"xmax\").text)\n",
    "                y_max = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "                # normalize 시켜주는 과정\n",
    "                # 1.0은 이미지의 너비 또는 높이를 1로 정규화 하는데 사용\n",
    "                print(img.size)\n",
    "                # size[0]: 폭\n",
    "                dx = int(img.size[0])\n",
    "                # size[1]: 높이\n",
    "                dy = int(img.size[1])\n",
    "                # x_center는 해당 객체의 bounding box 중심점의 x좌표\n",
    "                # 값을 YOLO 형식으로 bounding box 좌표를 변환하기 위해 사용\n",
    "                # 0 ~ 1 사이로 정규화 되어 있으므로 이 값을 YOLO 형식으로 변환 시에도 0 ~ 1\n",
    "                # y_center도 마찬가지의 원리\n",
    "                # x_center, y_center, w, h는 YOLO 형식으로 bounding box 좌표를 나타내는 값\n",
    "                # YOLO: 이미지를 Grid로 나누고 각 grid 셀마다 bounding box를 예측\n",
    "                # bounding box는 해당 셀에 대해 상대적인 위치와 크기로 표현\n",
    "                # 이를 위해 x_center, y_center, w, h를 사용한다\n",
    "                # x_center: bounding box 중심점의 x 좌표\n",
    "                x_center = ((x_min+x_max) / 2) / dx\n",
    "                # y_center: bounding box의 중심점의 y좌표\n",
    "                y_center = ((y_min+y_max) / 2) / dy\n",
    "                # bounding box의 너비(해당 셀의 너비를 기준으로 상대적인 크기)\n",
    "                w = (x_max - x_min) / img_width\n",
    "                # bounding box의 높이(해당 셀의 높이를 기준으로 한 상대적인 크기)\n",
    "                h = (y_max - y_min) / img_height\n",
    "\n",
    "                # YOLO 형식으로 변환된 Bounding box 정보를 txt 파일에 기록\n",
    "                # class_id: 해당 객체의 class id를 나타낸다.\n",
    "                # classes에 설정된 class 명의 index를 할당\n",
    "                # helmet, head, person의 클래스 ID를 할당\n",
    "                # x_center, y_center, w, h는 YOLO 형식으로 bounding box 좌표를 나타낸다.\n",
    "                f.write(f\"{class_id} {x_center:.5f} {y_center:.5f} {w:.5f} {h:.5f}\\n\")\n",
    "            print(\"Convert these files, {} file and {} file, into the YOLO format\".format(xml_file_name, image_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml 파일 제작\n",
    "# train, test, valid, number of classes, classes name 지정\n",
    "%cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\\\\archive\"\n",
    "data = {'train': \"./archive/images/train\",\n",
    "        'test': \"./archive/images/test\",\n",
    "        \"val\": \"./archive/images/valid\",\n",
    "        \"nc\": \"3\",\n",
    "        \"names\": ['helmet', 'head', 'person']}\n",
    "\n",
    "# data.yaml은 위의 dictionary 변수 명과 같이 한다.\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcde79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:\\\\Users\\\\user\\\\Desktop\\\\recognition_project\\\\yolov5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae21b0",
   "metadata": {},
   "source": [
    "YOLOv5 사용 결과\n",
    "1. yolov5s나 m이나 큰 차이는 없다.\n",
    "2. batch size 늘리고 epoch 수를 늘리려면 yolov5s를 사용하는 게 맞다.\n",
    "3. epoch 수는 최소 100 epoch을 진행해보고 hyperparameter를 조정하는 등의 과정을 거쳐야 할 듯 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후에 프로젝트 시작 후에 train, validation, test 진행\n",
    "# --conf 옵션은 confidence(신뢰도) 임계값을 설정하는 옵션이다.\n",
    "# batch 64 이상은 학습 중에 GPU 메모리 부족을 야기할 수 있다.\n",
    "!python train.py --img 416 --batch 16 --epochs 10 --data ./archive/data.yaml --cfg ./models/yolov5s.yaml --weights \"./runs/train/exp3/weights/best.pt\"  --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d4786",
   "metadata": {},
   "source": [
    "- Data Augmentation 진행\n",
    "- Fine Tuning 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa99788",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --source \"./archive/images/test\" --weights \"./runs/train/exp2/weights/best.pt\" --device cuda:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331da54",
   "metadata": {},
   "source": [
    "70 epoch 학습 결과\n",
    "- 헬멧을 쓴 사람이면 helmet으로 예측\n",
    "- 헬멧이 아닌 사람이면 head로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70aa52af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/exp3/weights/best.pt'], source=./data/images, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=cuda:0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v7.0-133-gcca5e21 Python-3.9.13 torch-1.13.1+cu116 CUDA:0 (GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\KakaoTalk_20230502_204707649.jpg: 640x480 1 head, 13.4ms\n",
      "image 2/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\bus.jpg: 640x480 3 heads, 10.0ms\n",
      "image 3/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\hard_hat_workers4000.png: 640x640 4 helmets, 9.0ms\n",
      "image 4/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\hard_hat_workers4001.png: 640x640 7 heads, 8.0ms\n",
      "image 5/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\hard_hat_workers4002.png: 640x640 4 helmets, 10.0ms\n",
      "image 6/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\hard_hat_workers4003.png: 640x640 3 helmets, 2 heads, 10.0ms\n",
      "image 7/7 C:\\Users\\user\\Desktop\\recognition_project\\yolov5\\data\\images\\tkwls.png: 448x640 4 heads, 13.5ms\n",
      "Speed: 0.8ms pre-process, 10.6ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source \"./data/images\" --weights \"./runs/train/exp3/weights/best.pt\" --device cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63de7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "picts_2 = os.listdir(\"./runs/detect/exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"./runs/detect/exp\"+\"\\\\\"+picts_2[386])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"./runs/detect/exp\"+\"\\\\\"+picts_2[756])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86821047",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8dbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
