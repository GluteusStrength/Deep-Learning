{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchsummary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfac618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 27 21:11:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.75       Driver Version: 462.75       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 306... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P8    13W /  N/A |    121MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b034578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Desktop\\deep learning\\Building_Segmentation\\open (2)\n"
     ]
    }
   ],
   "source": [
    "%cd open (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cea8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED 고정\n",
    "CFG = {\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 4,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d06994",
   "metadata": {},
   "source": [
    "### Data Augmentation 전에 우선 train, valid, test split\n",
    "- 5500: train, 1000: validation, 640: test\n",
    "- test_set을 통해 loss값 확인 및 DICE SCORE 산식 적용해서 확인할 예정\n",
    "- train에 대해서 직접 augmentation을 적용 시키고자 했으나 albumentaion으로 진행하는 것으로 변경했다.\n",
    "- 따로 이 부분은 활용하지 않을 예정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a2ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train_set = train[:6140]\n",
    "valid_set = train[6140:]\n",
    "train_set.to_csv(\"trainset.csv\")\n",
    "valid_set.to_csv(\"validset.csv\")\n",
    "# # # 2번째 테스트\n",
    "train2 = pd.read_csv(\"train.csv\")\n",
    "train_set = train2[1000:]\n",
    "valid_set = train2[:1000]\n",
    "train_set.to_csv(\"trainset2.csv\")\n",
    "valid_set.to_csv(\"validset2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False, num = None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        self.num = num\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.num == 0:\n",
    "            # csv파일을 불러와 column을 잘 보면 한 column은 파일의 저장 경로이다.\n",
    "            img_path = self.data.iloc[idx, 2]\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n",
    "            if self.infer:\n",
    "                if self.transform:\n",
    "                    image = self.transform(image=image)['image']\n",
    "                return image\n",
    "\n",
    "            mask_rle = self.data.iloc[idx, 3]\n",
    "            mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                image = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "\n",
    "            return image, mask\n",
    "        elif self.num == 1:\n",
    "            # csv파일을 불러와 column을 잘 보면 한 column은 파일의 저장 경로이다.\n",
    "            img_path = self.data.iloc[idx, 2]\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n",
    "            if self.infer:\n",
    "                if self.transform:\n",
    "                    image = self.transform(image=image)['image']\n",
    "                return image\n",
    "\n",
    "            mask_rle = self.data.iloc[idx, 3]\n",
    "            mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                image = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "\n",
    "            return image, mask\n",
    "        else:\n",
    "            img_path = self.data.iloc[idx, 1]\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n",
    "            if self.infer:\n",
    "                if self.transform:\n",
    "                    image = self.transform(image=image)['image']\n",
    "                return image\n",
    "\n",
    "            mask_rle = self.data.iloc[idx, 2]\n",
    "            mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                image = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "\n",
    "            return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc955893-22fd-4320-88be-7aa0d790cbd9",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "- 괜찮은 transform 조합을 찾는다.\n",
    "- transform 조합을 통해 더 나은 결과를 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_base = A.Compose(\n",
    "    [   \n",
    "        A.augmentations.crops.transforms.CropNonEmptyMaskIfExists(height = 224, width = 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "# 기본적인 augmentation: random crop, horizontal, vertical, rotate, blur, normalize\n",
    "transform_train_aug = A.Compose(\n",
    "    [   \n",
    "        A.augmentations.crops.transforms.CropNonEmptyMaskIfExists(height = 224, width = 224),\n",
    "        A.OneOf([A.HorizontalFlip(p = 0.5), A.VerticalFlip(p = 0.5)], p = 0.8),\n",
    "        A.Rotate(limit = 180, p = 0.5, border_mode = cv2.BORDER_REPLICATE),\n",
    "        A.GaussianBlur(blur_limit = (3, 7), always_apply = False, p = 0.5),\n",
    "        A.GridDropout(ratio = 0.2, random_offset = True, holes_number_x=4,\n",
    "                      holes_number_y= 4,p = 0.8),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# transform_train_aug2 = A.Compose(\n",
    "#     [   \n",
    "#         A.CenterCrop(224, 224),\n",
    "#         A.OneOf([A.HorizontalFlip(p = 0.5), A.VerticalFlip(p = 0.5)]),\n",
    "# #         A.Rotate(limit = 180, p = 0.5, border_mode = cv2.BORDER_REPLICATE),\n",
    "#         A.OneOf([A.GaussianBlur(blur_limit = (3, 7), always_apply = False, p = 0.5), A.GaussNoise()]),\n",
    "#         A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "transform_other = A.Compose(\n",
    "    [   \n",
    "#         A.augmentations.crops.transforms.CropNonEmptyMaskIfExists(height = 224, width = 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transform_other1 = A.Compose(\n",
    "    [   \n",
    "        A.RandomCrop(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "# transform_other2 = A.Compose(\n",
    "#     [   \n",
    "#         A.RandomCrop(224, 224),\n",
    "#         A.Normalize(),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "# transform_version2 = A.Compose(\n",
    "#     [   \n",
    "#         A.CenterCrop(224, 224),\n",
    "# #         A.HorizontalFlip(p = 0.5),\n",
    "#         A.Rotate(limit = 180, p = 0.8, border_mode = cv2.BORDER_REPLICATE),\n",
    "# #         A.VerticalFlip(p = 0.5),\n",
    "# #         A.HueSaturationValue(hue_shift_limit = 20, sat_shift_limit = 30, val_shift_limit = 20, p = 0.5),\n",
    "#         A.GaussianBlur(blur_limit = (3, 7), always_apply = False, p = 0.5),\n",
    "#         A.Normalize(),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# pretrained 된 것 더 이어서 학습 시키는 용도\n",
    "# 이럴 경우는 주의해서 load 시킬 것\n",
    "# crop_transform = A.Compose(\n",
    "#     [   \n",
    "#         A.RandomCrop(224, 224),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5),\n",
    "#         A.RandomRotate90(p=0.5),\n",
    "#         A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "#         A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#         A.CLAHE(clip_limit=4.0, tile_grid_size=(8,8), p=0.5),\n",
    "#         A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ToTensorV2()\n",
    "#     ]\n",
    "# )\n",
    "train_dataset_base = SatelliteDataset(csv_file='./trainset.csv', transform = transform_train_base, num = 0)\n",
    "train_dataset_aug = SatelliteDataset(csv_file = \"./trainset.csv\", transform = transform_train_aug, num = 0)\n",
    "# train_set = SatelliteDataset(csv_file = \"./train.csv\", transform = crop_transform, num = 0)\n",
    "# train_dataset_aug2 = SatelliteDataset(csv_file = \"./trainset.csv\", transform = transform_train_aug2, num = 0)\n",
    "train_dataset = train_dataset_base + train_dataset_aug\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "valid_dataset = SatelliteDataset(csv_file=\"./validset.csv\", transform = transform_other1, num = 1)\n",
    "valid_dataset = valid_dataset\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = CFG[\"batch_size\"], shuffle = False, num_workers = 0)\n",
    "# test_dataset = SatelliteDataset(csv_file = \"./testset.csv\", transform = transform_other, num = 1)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = CFG[\"batch_size\"], shuffle = False, num_workers = 0)\n",
    "# validation 추가\n",
    "# train_dataset = SatelliteDataset(csv_file = \"./train.csv\", transform = transform_train_base, num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42501fc-b573-4893-a7c4-5e280dfdaf09",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "- CVPR 2020에 발표된 Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured Environment 활용\n",
    "- 총 7개의 MBConv Block을 이용\n",
    "- 논문에서 제시한 block을 통과한 후의 channel와 pretrained를 위한 channel의 수의 차이가 존재\n",
    "- encoder 부분은 이를 참고해 EfficientNet_v2_m을 적용.\n",
    "- decoder 부분은 U_Net의 decoder 활용.\n",
    "- decoder 부분을 변형 시킬 예정\n",
    "    - 특히 단순히 U-Net이 아닌 U-Net++, U-Net3+의 아이디어도 참고할 예정\n",
    "\n",
    "\n",
    "※ 추가적으로 customized loss, DICE Score 확인해가며 진행할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65960bfb-803a-4c40-b713-6f647779e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "# U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n",
    "# 기본 model에는 BatchNormalization이 존재하지 않으므로 따로 정의했다.\n",
    "# U_Net Decoder의 마지막 부분 참고하여 Conv2d(32, 1)로 변형했다.\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "# EFFICIENTNET-V2 ENCODER + U_NET DECODER\n",
    "# pretrained_task를 이용할 예정.\n",
    "\n",
    "class EFFv2_UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EFFv2_UNet, self).__init__()\n",
    "        \n",
    "        # EfficientNet_V2_L + U_Net decoder\n",
    "        self.backbone = models.efficientnet_v2_m(pretrained = True, weights='EfficientNet_V2_M_Weights.DEFAULT')\n",
    "        # block 설정\n",
    "        self.feature1 = self.backbone.features[0]\n",
    "        self.feature2 = self.backbone.features[1]\n",
    "        self.feature3 = self.backbone.features[2]\n",
    "        self.feature4 = self.backbone.features[3]\n",
    "        self.feature5 = self.backbone.features[4]\n",
    "        self.feature6 = self.backbone.features[5]\n",
    "        self.feature7 = self.backbone.features[6]\n",
    "\n",
    "        # upsample은 6회 진행.\n",
    "        # 해당 코드는 이미지나 tensor의 크기를 2배로 늘려주는 역할을 한다.\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        # concatenate: 512 channels, 256 channels\n",
    "        # 7 -> 14\n",
    "        self.dconv_up6 = double_conv(304, 512)\n",
    "        self.dconv_up5 = double_conv(512 + 176, 256)\n",
    "        # concatenate: 256 channels, 128 channels\n",
    "        # 14 -> 28\n",
    "        self.dconv_up4 = double_conv(256 + 160, 128)\n",
    "        # concatenate: 128 channels, 64 channels\n",
    "        # 28 -> 56\n",
    "        self.dconv_up3 = double_conv(128 + 80, 64)\n",
    "        # 56 -> 112\n",
    "        self.dconv_up2 = double_conv(64 + 48, 64)\n",
    "        # 112 -> 224\n",
    "        self.dconv_up1 = double_conv(64 + 24, 32)\n",
    "        # last process\n",
    "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # efficientnet의 모든 block을 통과한다.\n",
    "        # convolution 저장(추후에 concatenate)\n",
    "        conv1 = self.feature1(x)\n",
    "        x = self.feature1(x)\n",
    "        conv2 = self.feature2(x)\n",
    "        x = self.feature2(x)\n",
    "        conv3 = self.feature3(x)\n",
    "        x = self.feature3(x)\n",
    "        conv4 = self.feature4(x)\n",
    "        x = self.feature4(x)\n",
    "        conv5 = self.feature5(x)\n",
    "        x = self.feature5(x)\n",
    "        conv6 = self.feature6(x)\n",
    "        x = self.feature6(x)\n",
    "        # 이 자체가 upsampling + conv2d를 지난다.\n",
    "        x = self.feature7(x)\n",
    "        \n",
    "        # 논문에 따르면 우선 마지막 block의 결과는 UpSampling 후 Conv2D를 거친다\n",
    "        # 논문과는 약간 결이 다르지만 일단 전의 정보를 최대한 반영해서 학습을 진행하도록 했다.\n",
    "        x = self.dconv_up6(x)\n",
    "        # size가 14로 된다.\n",
    "        x = self.upsample(x)\n",
    "        # conv6의 경우는 concatenate된다\n",
    "        # x의 차원은 512, conv6는 112\n",
    "        x = torch.cat([x, conv6], dim=1)\n",
    "        # 여기까지 하면 size는 14, channel 수는 512 + 112\n",
    "        x = self.dconv_up5(x)\n",
    "        # 이를 통과하면 x는 256이 된다\n",
    "        x = torch.cat([x, conv5], dim=1)\n",
    "        # 28\n",
    "        x = self.upsample(x)\n",
    "        x = self.dconv_up4(x)\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        # 56\n",
    "        x = self.upsample(x)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        # 112\n",
    "        x = self.upsample(x)\n",
    "        x = self.dconv_up2(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        # 112\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.upsample(x) \n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        # 따로 sigmoid를 통과하지는 않는다.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4f53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    # gradient 추적 x\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(valid_loader):\n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss = val_loss / len(valid_loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63efb381-98c6-4d9b-a3b6-bd11c7fa8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 초기화\n",
    "# model = EFFv2_UNet().to(device)\n",
    "model = EFFv2_UNet()\n",
    "model.load_state_dict(torch.load(\"./0.2557478419079834_checkpoint.pt\"))\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, valid_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    # training loop\n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    x_range = range(1, CFG[\"epochs\"]+1)\n",
    "    # for early_stopping\n",
    "    PATIENCE = 7\n",
    "    cnt = 0\n",
    "    for epoch in range(CFG[\"epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        valid_loss = validation(model, valid_loader, criterion, device)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_loss)\n",
    "        \n",
    "        if best_loss > valid_loss:\n",
    "            best_loss = valid_loss\n",
    "            # validation 기준으로 최적의 모델 저장\n",
    "            cnt = 0\n",
    "            best_model = model\n",
    "            print(\"validation_loss의 minimum epoch 시기 {}, val_loss값 {}\".format(epoch+1, valid_loss))\n",
    "        else:\n",
    "            cnt += 1\n",
    "        if cnt > PATIENCE:\n",
    "            print(\"Validation loss renewal stopped\")\n",
    "            return best_model\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79027a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice score와 cross entropy loss를 결합한다. \n",
    "class DiceCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceCrossEntropyLoss, self).__init__()\n",
    "        # Binary Cross Entropy logloss 모듈\n",
    "        self.BCELoss = torch.nn.BCEWithLogitsLoss()\n",
    "    def forward(self, outputs, targets, smooth=1):\n",
    "        # binary cross entropy logloss 계산\n",
    "        BCELOSS = self.BCELoss(outputs, targets)\n",
    "        # dice score 계산하는 부분\n",
    "        # torch.sigmoid 또는 torch.nn.function의 sigmoid를 활용 가능하다.\n",
    "        inputs = torch.sigmoid(outputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        # 기본적인 dice score 계산(smooth factor는 분모가 0이 되는 것을 방지)\n",
    "        dice = (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        # dice의 경우는 1에 근접할 수록 좋은 score이다. 따라서 loss로 활용하고자 한다면 반대로 진행\n",
    "        # loss가 0으로 수렴할 수도 있어서 보통 변수 결합 하여 모든 것을 고려할 때는 곱하지만 더하는 방향으로 진행했습니다.\n",
    "        combined_loss = (1-dice) + BCELOSS\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3c59bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [20:02<00:00,  2.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:44<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 1, val_loss값 0.24833990561962127\n",
      "Epoch 1, Loss: 0.2928720819469578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:28<00:00,  2.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:38<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 2, val_loss값 0.24679794321954251\n",
      "Epoch 2, Loss: 0.2781615990393034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:08<00:00,  2.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.26905114729721885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:05<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.2700476332916499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:56<00:00,  2.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 6, val_loss값 0.24352813100814819\n",
      "Epoch 6, Loss: 0.26787863300221365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:55<00:00,  2.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.2638328046917139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:50<00:00,  2.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 8, val_loss값 0.24322365793585776\n",
      "Epoch 8, Loss: 0.2632879401578383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:56<00:00,  2.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.26151258485567686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:04<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.26289466306664266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:07<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.2594830796971966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:02<00:00,  2.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 12, val_loss값 0.23559779271483422\n",
      "Epoch 12, Loss: 0.2584675058504075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [19:05<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.2638616055832624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:53<00:00,  2.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.2597230375346998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:47<00:00,  2.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.25425011136293024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:46<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss의 minimum epoch 시기 16, val_loss값 0.22917184776067734\n",
      "Epoch 16, Loss: 0.25697383620208947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:46<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.25943315546801893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:45<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.26021576741716373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:41<00:00,  2.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.25862688827009855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:43<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 20, Loss: 0.2561519536866233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:43<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.25351706823692066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:44<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.2563194066650122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:45<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.252709308669505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3070/3070 [18:43<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:36<00:00,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Validation loss renewal stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 기준은 best validation model을 기준으로 설정.\n",
    "# loss function과 optimizer 정의\n",
    "# train_loss: 0.27아래, validation loss: 0.25 아래로 나올 수 있도록 계속해서 실험할 예정. validation 기준으로 dice score 예상: 74점\n",
    "criterion = DiceCrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(new_model.parameters(),lr=CFG[\"learning_rate\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = CFG[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience = 3, threshold_mode = 'abs', min_lr = 1e-5, verbose = True)\n",
    "best_model = train(model, optimizer, criterion, train_loader, valid_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8651f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt file을 save\n",
    "torch.save(best_model, \"./best_model_efficientunetv2m_4batch_augmentation_7_28_dicescore.pth\")\n",
    "torch.save(model, \"./last_model_7_28.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5efad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDataset(csv_file='./test.csv', transform=transform_other1, infer=True, num = 2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355b431c-ac8e-4c40-9046-4d53e4bab14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 15160/15160 [15:32<00:00, 16.25it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    best_model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        \n",
    "        outputs = best_model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2cbbb-04f1-4f9c-b4df-4b744dfce046",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6543d00-32b3-4f2d-a572-d0879fd0a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d477f",
   "metadata": {},
   "source": [
    "### try 1 (7/4)\n",
    "- 30 epochs\n",
    "- 데이터를 4500장, 1500장, 1140장으로 나눠서 진행\n",
    "- train에 해당하는 4500장 중 1000장에 대해서 noise를 추가해서 진행\n",
    "- Compose의 경우는 단순히 224x224로 Resize 진행\n",
    "- random seed는 고정하지 않았음\n",
    "- pt file을 저장.\n",
    "### try 2 (7/5)\n",
    "- try 1에서의 pt file을 불러와서 진행\n",
    "- train은 앞의 5000장, validation은 1500장으로 진행\n",
    "- random seed 값은 42로 고정\n",
    "- Compose의 경우는 단순히 Albumentation의 RandomCrop으로 진행\n",
    "- 20 epoch 추가 수행\n",
    "- validation loss는 0.07, train loss는 0.08\n",
    "- pt file을 저장\n",
    "### try 3(7/6)\n",
    "- 50 epochs\n",
    "- train set은 5000장, noise augmentation은 500장, flip, horizontal flip을 train에 적용\n",
    "- 1500장의 validation set\n",
    "- validation loss는 0.07, train\n",
    "\n",
    "※ U-Net 역시 decoder 부분에 대한 pretrained task weight를 넣어 진행해 볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99604d8",
   "metadata": {},
   "source": [
    "### final\n",
    "- 최종적으로 얼마나 잘 test 데이터에 대해서 체크를 하는 지 확인해 볼 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c141a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
