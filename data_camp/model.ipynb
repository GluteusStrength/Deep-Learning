{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"T4VD4-C-Sgqc"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch import optim\n","import os\n","import gc\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tqdm\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from matplotlib import pyplot as plt\n","import PIL.Image as pilimg\n","from torch.autograd import Variable\n","from sklearn.metrics import plot_confusion_matrix, classification_report"]},{"cell_type":"markdown","metadata":{"id":"qpnN4W7wwXh-"},"source":["Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AjD_0nUwXOx"},"outputs":[],"source":["def valid(data_loader, model):\n","    \"\"\" model inference \"\"\"\n","    n_predict = 0\n","    n_correct = 0\n","    with torch.no_grad():\n","        for X, Y in tqdm.tqdm(data_loader):\n","            X, Y = X.cuda(), Y.cuda()\n","            y_hat = model(X)\n","            y_hat.argmax()\n","\n","            _, predicted = torch.max(y_hat, 1)\n","            \n","            n_predict += len(predicted)\n","\n","            predicted = predicted.unsqueeze(dim=1)\n","            correct = (Y == predicted).squeeze(dim=-1)\n","            n_correct += int(correct.sum())\n","\n","    accuracy = n_correct/n_predict\n","    print(f\"Accuracy: {accuracy} ()\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiyv-twpKlM7"},"outputs":[],"source":["def test(data_loader, model):\n","    \"\"\" model inference \"\"\"\n","    n_predict = 0\n","    n_correct = 0\n","    y_pred = []\n","    y_label = []\n","    model.load('/content/drive/MyDrive/Data/model/batch32_lr1e-4_epoch95.pt')\n","    with torch.no_grad():\n","        for X, Y in tqdm.tqdm(data_loader):\n","            X, Y = X.cuda(), Y.cuda()\n","            y_hat = model(X)\n","            y_hat.argmax()\n","            \n","            _, predicted = torch.max(y_hat, 1)\n","            \n","            n_predict += len(predicted)\n","            predicted = predicted.unsqueeze(dim=1)\n","            correct += (Y == predicted).squeeze(dim=-1)\n","            n_correct += int(correct.sum())\n","            \n","    accuracy = n_correct/n_predict\n","    print(f\"Accuracy: {accuracy} ()\")\n","\n","    print(classification_report(y_label, y_pred))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b6rT1owJTCQm"},"source":["Model\n","- based on VGGNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhv4cfV5qCX3"},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.conv_layer1 = torch.nn.Sequential(   #224*224*3\n","            torch.nn.Conv2d(1, 64, kernel_size=3, padding=1),   #224x224x64\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.conv_layer2 = torch.nn.Sequential(   \n","            torch.nn.Conv2d(64, 128, kernel_size=3 , padding=1),   #112x112x128\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n","        )  \n","\n","        self.conv_layer3 = torch.nn.Sequential(\n","            torch.nn.Conv2d(128, 256, kernel_size=3 , padding=1),   #56x56x256\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.conv_layer4 = torch.nn.Sequential(\n","            torch.nn.Conv2d(256, 512, kernel_size=3 , padding=1),   #28x28x512\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.conv_layer5 = torch.nn.Sequential(\n","            torch.nn.Conv2d(512, 512, kernel_size=3 , padding=1),   #14x14x512\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),   #-> 7x7x512로 변화\n","        )\n","\n","        self.fc6 = torch.nn.Linear(7*7*512, 4096, bias = True)\n","        torch.nn.init.xavier_uniform_(self.fc6.weight)\n","        self.layer6 = torch.nn.Sequential(\n","            self.fc6,\n","            torch.nn.ReLU(),\n","            torch.nn.BatchNorm1d(4096),\n","            torch.nn.Dropout(p=0.15)\n","        )\n","\n","        self.fc7 = torch.nn.Linear(4096, 4096, bias = True)\n","        torch.nn.init.xavier_uniform_(self.fc7.weight)\n","        self.layer7 = torch.nn.Sequential(\n","            self.fc7,\n","            torch.nn.ReLU(),\n","            torch.nn.BatchNorm1d(4096),\n","            torch.nn.Dropout(p=0.15)\n","        )\n","\n","        self.fc8 = torch.nn.Linear(4096, 1000, bias = True)\n","        torch.nn.init.xavier_uniform_(self.fc8.weight)\n","        self.layer8 = torch.nn.Sequential(\n","            self.fc8,\n","            torch.nn.ReLU(),\n","            torch.nn.BatchNorm1d(1000),\n","            torch.nn.Dropout(p=0.15)\n","        )\n","\n","        self.fc9 = torch.nn.Linear(1000, 20, bias = True)\n","        torch.nn.init.xavier_uniform_(self.fc9.weight)\n","    \n","        for m in self.modules():\n","            if isinstance(m, torch.nn.Linear):\n","                torch.nn.init.uniform_(m.weight.data)\n","\n","    def forward(self, x):\n","        out = self.conv_layer1(x)\n","        out = self.conv_layer2(out)\n","        out = self.conv_layer3(out)\n","        out = self.conv_layer4(out)\n","        out = self.conv_layer5(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.layer6(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.fc9(out)\n","        return out\n","\n","\n","model = Net()\n","model = model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWyLyaMzzePJ"},"outputs":[],"source":["saved_model_path = ('/content/drive/MyDrive/Data/model/batch32_lr1e-4_epoch95.pt')\n","\n","model = torch.load(saved_model_path)"]},{"cell_type":"markdown","metadata":{"id":"B0PcnHKHut7d"},"source":["Parameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSD-ubH2utoa"},"outputs":[],"source":["model_path = ('/content/drive/MyDrive/Data/model/batch16_lr1e-4_epoch')\n","\n","plt_path = ('/content/drive/MyDrive/Data/learning_curve/batch16_lr1e-4_3rdtime.png')\n","\n","data_batch_size = 32\n","\n","criterion = torch.nn.CrossEntropyLoss().cuda()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","\n","training_epochs = 10000"]},{"cell_type":"markdown","metadata":{"id":"6MP945SqTGu7"},"source":["Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xdIwIZ_zAb8"},"outputs":[],"source":["# train_x = np.load('/content/drive/MyDrive/Data/Dataset/train_x.npy')\n","# train_y = np.load('/content/drive/MyDrive/Data/Dataset/train_y.npy')\n","\n","# valid_x = np.load('/content/drive/MyDrive/Data/Dataset/validation_x.npy')\n","# valid_y = np.load('/content/drive/MyDrive/Data/Dataset/validation_y.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557,"status":"ok","timestamp":1667133704123,"user":{"displayName":"Machine Running Learning","userId":"02190580932973673985"},"user_tz":-540},"id":"2MvU2Z3hzA8b","outputId":"032413ea-c251-4ec5-870e-3d9922d7d190"},"outputs":[{"name":"stdout","output_type":"stream","text":["(21916, 1, 224, 224) (21916, 1) (1994, 1, 224, 224) (1994, 1)\n"]}],"source":["# print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4XrwLyoy2Uv"},"outputs":[],"source":["train_x = np.load('/content/drive/MyDrive/Data/Dataset/train_x.npy')\n","train_y = np.load('/content/drive/MyDrive/Data/Dataset/train_y.npy')\n","\n","valid_x = np.load('/content/drive/MyDrive/Data/Dataset/validation_x.npy')\n","valid_y = np.load('/content/drive/MyDrive/Data/Dataset/validation_y.npy')\n","\n","train_x = torch.from_numpy(train_x).float()\n","train_y = torch.from_numpy(train_y).long()\n","\n","valid_x = torch.from_numpy(valid_x).float()\n","valid_y = torch.from_numpy(valid_y).long()\n","\n","train_data = torch.utils.data.TensorDataset(train_x, train_y)\n","valid_data = torch.utils.data.TensorDataset(valid_x, valid_y)\n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=data_batch_size, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=data_batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oetCEv8BMW1H"},"outputs":[],"source":["test_x = np.load('/content/drive/MyDrive/Data/Dataset/test_x.npy')\n","test_y = np.load('/content/drive/MyDrive/Data/Dataset/test_y.npy')"]},{"cell_type":"markdown","metadata":{"id":"DSb1Zb4yvWLb"},"source":["Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNvZenzwupdr"},"outputs":[],"source":["torch.autograd.set_detect_anomaly(True)\n","\n","losses = []\n","before_model_path = 'Null'\n","\n","for epoch in range(training_epochs):\n","    model.train()\n","    cost = 0\n","    n_batches = 0\n","    for X, y in tqdm.tqdm(train_loader):\n","        X, y = X.cuda(), y.cuda()\n","        optimizer.zero_grad()\n","        y_hat = model(X)\n","        y = y.squeeze(dim=-1)\n","        loss = criterion(y_hat, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        cost += loss.item()\n","        n_batches += 1\n","\n","    cost /= n_batches\n","    losses.append(cost)\n","    print('[Epoch: {:>4} cost = {:>.9}'.format(epoch+1, cost))\n","    print(\"Dev\")\n","    valid(valid_loader, model)\n","    if min(losses) == cost:\n","        if os.path.exists(before_model_path):\n","            os.remove(before_model_path)\n","        after_model_path = model_path + str(epoch+1) + '_3rdtime.pt'\n","        before_model_path = after_model_path\n","        print('save model')\n","        torch.save(model, after_model_path)\n","\n","    plt.plot(losses)\n","    plt.savefig(plt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqXH6FisCiSf"},"outputs":[],"source":["# import numpy as np\n","# data = np.load('/content/drive/MyDrive/Data/Dataset/test_x.npy')\n","# data_y = np.load('/content/drive/MyDrive/Data/Dataset/test_y.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667023099914,"user":{"displayName":"Machine Running Learning","userId":"02190580932973673985"},"user_tz":-540},"id":"DhMSBLopTkBg","outputId":"4abd522d-ed56-4b85-9caa-e367540b5c23"},"outputs":[{"data":{"text/plain":["2026"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# len(data_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667023108294,"user":{"displayName":"Machine Running Learning","userId":"02190580932973673985"},"user_tz":-540},"id":"ts-NtZA2T_fA","outputId":"685af5a8-ea03-4e01-c997-8a64f058e61d"},"outputs":[{"data":{"text/plain":["2026"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twCRmMoVUfSr"},"outputs":[],"source":["# def test(data_loader, model):\n","#     \"\"\" model inference \"\"\"\n","#     n_predict = 0\n","#     n_correct = 0\n","#     model.load_state_dict(torch.load(\"content/drive/MyDrive/Data/Dataset/model3.pt\"))\n","#     with torch.no_grad():\n","#         for X, Y in tqdm.tqdm(data_loader):\n","#             X, Y = X.cuda(), Y.cuda()\n","#             y_hat = model(X)\n","#             y_hat.argmax()\n","            \n","#             _, predicted = torch.max(y_hat, 1)\n","            \n","#             n_predict += len(predicted)\n","#             n_correct += (Y == predicted).sum()\n","            \n","#     accuracy = n_correct/n_predict\n","#     print(f\"Accuracy: {accuracy} ()\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALdEot-LmWAm"},"outputs":[],"source":["def freeze(model):\n","    for p in model.parameters():\n","        p.requires_grad = False\n","\n","    return model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XD7faZhPyAJM"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMZrA7+BFNZ4MbmoxJ4Ccr3","machine_shape":"hm","mount_file_id":"1Rek8FIS-xshC_hHNRf8XvY1ofoojyMsQ","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
