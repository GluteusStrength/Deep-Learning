{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4b15b9-efc3-48e4-916b-e720d734feeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlrkrwlsmd/.local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "class simCLR(nn.Module):\n",
    "    def __init__(self, embedding_size, model):\n",
    "        super(simCLR, self).__init__()\n",
    "        self.backbone = model\n",
    "        self.fc = nn.Linear(1024 * 7 * 7, embedding_size)\n",
    "        torch.nn.init.kaiming_normal_(self.fc.weight)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.reshape(-1, 1024 * 7 * 7)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Feature extraction layer.\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, (9, 9), (1, 1), (4, 4)),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Non-linear mapping layer.\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, (5, 5), (1, 1), (2, 2)),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Rebuild the layer.\n",
    "        self.reconstruction = nn.Conv2d(32, 3, (5, 5), (1, 1), (2, 2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    # Support torch.script function.\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.features(x)\n",
    "        out = self.map(out)\n",
    "        out = self.reconstruction(out)\n",
    "\n",
    "        return out\n",
    "# srcnn = torch.load(\"best_psnr.pt\", map_location = \"cpu\")\n",
    "model_ssl = torch.load(\"res_ssl_10_.pt\", map_location = \"cpu\")\n",
    "model_head = timm.create_model(\"convnext_base.fb_in22k\", pretrained = True, num_classes = 25).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36919ced-d052-4e88-b437-30568b3e1639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extraction = nn.Sequential(*list(model_ssl.children())[:-2])\n",
    "# for param in extraction.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4d9a43-57ec-48a8-94fd-4ce586d07c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class convnext_base(nn.Module):\n",
    "    def __init__(self, extraction, head, srcnn):\n",
    "        super(convnext_base, self).__init__()\n",
    "        # self.srcnn = srcnn\n",
    "        self.extraction = extraction\n",
    "        self.head = head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.srcnn(x)\n",
    "        x = self.extraction(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "model = convnext_base(extraction = extraction, head = model_head, srcnn = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8426bb55-5325-4e85-96e2-bca601ed283a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"DEVICE\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06636100-09ec-4ac1-b1c7-0ab7808a954d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageSet(Dataset):\n",
    "    def __init__(self, img, transform = None, class_name = None, label = None):\n",
    "        self.img = img\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.class_name = class_name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx]\n",
    "        label = self.label[idx]\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = class_name[label]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9f923b-2bf1-44b2-80f0-f4a865320bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.Resize([224, 224])                    \n",
    "])\n",
    "data = pd.read_csv(\"train_.csv\")\n",
    "trainset, valset, _, _ = train_test_split(data, data[\"label\"], test_size = 0.2, stratify = data[\"label\"], random_state = 0)\n",
    "_, augmentset, _, _ = train_test_split(trainset, trainset[\"label\"], test_size = 0.5, stratify = trainset[\"label\"], random_state = 0)\n",
    "trainset = trainset.reset_index()\n",
    "trainset.drop([\"index\"], axis = 1, inplace = True)\n",
    "valset = valset.reset_index()\n",
    "valset.drop([\"index\"], axis = 1, inplace = True)\n",
    "augmentset = augmentset.reset_index()\n",
    "augmentset.drop([\"index\", \"Unnamed: 0\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ff2c59-1324-49ae-a60b-a502bc55df1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = np.unique(data[\"label\"])\n",
    "class_name = {name: i for i, name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b760fe6-9bed-4888-9de8-638d56acbca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = ImageSet(img = trainset[\"img_path\"], transform = transform, class_name = class_name, label = trainset[\"label\"])\n",
    "validset = ImageSet(img = valset[\"img_path\"], transform = transform, class_name = class_name, label = valset[\"label\"])\n",
    "# augment_set = ImageSet(img = augmentset[\"upscale_img_path\"], transform = transform, class_name = class_name, label = augmentset[\"label\"])\n",
    "# trainset = train_set + augment_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa61483-0913-4a04-8049-1bdfcf0e2744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43935df6-e48d-41fb-af1d-777ae07d0cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_set, batch_size = CFG[\"BATCH_SIZE\"], shuffle = True, num_workers = 0)\n",
    "validloader = DataLoader(validset, batch_size = CFG[\"BATCH_SIZE\"], shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0c9112-53f3-49c3-9f13-44fa5060abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, j in trainloader:\n",
    "#     print(i.size())\n",
    "#     print(j.size())\n",
    "#     print(model(i).size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f36489-99df-4067-aefe-ed599a26bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,\\\n",
    "    threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for imgs, labels in tqdm(iter(train_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 Score : [{_val_score:.5f}]')\n",
    "       \n",
    "        if scheduler is not None:\n",
    "            # validation score를 기준으로 scheduler를 조정한다\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}], Best Val F1 Score : [{_val_score:.5f}]')\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618b5b3a-d05d-4d26-9723-b35bd89356ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(iter(val_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = f1_score(true_labels, preds, average='macro')\n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14413ae-598f-4da7-bd45-25120750204d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [04:00<00:00,  1.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:14<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.53563] Val Loss : [0.19516] Val F1 Score : [0.94330]\n",
      "Epoch [1], Train Loss : [0.53563], Best Val F1 Score : [0.94330]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:43<00:00,  1.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:14<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.08084] Val Loss : [0.17839] Val F1 Score : [0.94869]\n",
      "Epoch [2], Train Loss : [0.08084], Best Val F1 Score : [0.94869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▎                                                                                         | 18/396 [00:10<03:25,  1.84it/s]"
     ]
    }
   ],
   "source": [
    "infer_model = train(model, optimizer, trainloader, validloader,\\\n",
    "    scheduler, device = CFG[\"DEVICE\"])\n",
    "torch.save(infer_model, \"best_model_CONVNEXT_simclr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14f24c-4ad4-44be-82ec-929e9ebda089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "    def __init__(self, img, transform = None):\n",
    "        self.img = img\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx]\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "transform_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.Resize([224, 224])                    \n",
    "])\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_set = TestSet(img = test[\"img_path\"], transform = transform_)\n",
    "test_loader = DataLoader(test_set, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb3b31-8c9d-42e2-bb5b-268be9acd88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            pred = model(imgs)\n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98298045-9d94-4870-8a0f-4ce5d39423e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = inference(model, test_loader, device = CFG[\"DEVICE\"])\n",
    "classes = list(class_name.keys())\n",
    "final = []\n",
    "for pred in preds:\n",
    "    final.append(classes[pred])\n",
    "submit = pd.read_csv(\"./sample_submission.csv\")\n",
    "submit[\"label\"] = final\n",
    "submit.to_csv(\"./submit_30epochs_large.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f090e1-f6f5-4b8d-bfbd-52e136aeecab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
