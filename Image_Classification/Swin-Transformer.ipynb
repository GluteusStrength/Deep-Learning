{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0171084d-96dc-417b-a39b-0b2870c50e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlrkrwlsmd/.local/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CFG = {\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"SEED\": 42,\n",
    "    \"DEVICE\": torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"MODEL\":  \"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\"\n",
    "}\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# torch.cuda.set_device(1)\n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352adc7b-cea4-4ae5-ae62-d4950229f51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG[\"SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d34f4dc-3c41-43d7-976b-b901c56f63da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_.csv\")\n",
    "# Class Name 지정\n",
    "classes = np.unique(data[\"label\"])\n",
    "class_name = {name: i for i, name in enumerate(classes)}\n",
    "trainset, valset, _, _ = train_test_split(data, data[\"label\"], test_size = 0.1, stratify = data[\"label\"], random_state = 42)\n",
    "trainset = trainset.reset_index()\n",
    "trainset.drop([\"index\", \"Unnamed: 0\"], axis = 1, inplace = True)\n",
    "valset = valset.reset_index()\n",
    "valset.drop([\"index\", \"Unnamed: 0\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869cad58-ea78-4c27-b3d4-8548337039bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path = CFG[\"MODEL\"]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size = (image_processor.size[\"height\"], image_processor.size[\"width\"])),\n",
    "    transforms.Normalize(\n",
    "        mean = image_processor.image_mean,\n",
    "        std = image_processor.image_std\n",
    "    )\n",
    "])\n",
    "\n",
    "class ImageSet(Dataset):\n",
    "    def __init__(self, img_low, img_high, transform = None, class_name = None, label = None):\n",
    "        self.img_low = img_low\n",
    "        self.img_high = img_high\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.class_name = class_name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        images_low = self.img_low[idx]\n",
    "        images_high = self.img_high[idx]\n",
    "        label = self.label[idx]\n",
    "        imgs_low = cv2.imread(images_low)\n",
    "        imgs_low = cv2.cvtColor(imgs_low, cv2.COLOR_BGR2RGB)\n",
    "        imgs_high = cv2.imread(images_high)\n",
    "        imgs_high = cv2.cvtColor(imgs_high, cv2.COLOR_BGR2RGB)\n",
    "        # if self.transform:\n",
    "        #     image_low = self.transform(imgs_low)\n",
    "        #     image_high = self.transform(imgs_high)\n",
    "        label = class_name[label]\n",
    "        return imgs_low, label\n",
    "\n",
    "def collator(data, transform):\n",
    "    imgs, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(img) for img in imgs])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    macro_f1 = metric.compute(\n",
    "        predictions = predictions, references = labels, average = \"macro\"\n",
    "    )\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9fb672-7745-4a21-b84b-8de1af27dc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init(classes, class_name):\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        CFG[\"MODEL\"],\n",
    "        num_labels = len(classes),\n",
    "        id2label = {idx: label for label, idx in class_name.items()},\n",
    "        label2id = class_name,\n",
    "        ignore_mismatched_sizes = True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c425a4-bbe0-47bd-b223-3ba2535e4823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainset = ImageSet(img_low = trainset[\"img_path\"], img_high = trainset[\"upscale_img_path\"], transform = transform, class_name = class_name, label = trainset[\"label\"])\n",
    "validset = ImageSet(img_low = valset[\"img_path\"], img_high = valset[\"upscale_img_path\"], transform = transform, class_name = class_name, label = valset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae93d8f-40d3-4247-8c9b-78cfa940bb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = CFG[\"DEVICE\"]\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./swin-transformer\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    num_train_epochs = 50,\n",
    "    weight_decay = 1e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=400,\n",
    "    seed = CFG[\"SEED\"],\n",
    "    warmup_ratio = 0.1,\n",
    "    label_smoothing_factor = 1e-3,\n",
    "    remove_unused_columns = False,\n",
    ")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c69661-68ea-4b40-9183-2adf364728ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 1536]) in the checkpoint and torch.Size([25, 1536]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([25]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1539' max='22250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1539/22250 38:36 < 8:40:11, 0.66 it/s, Epoch 3.45/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.058000</td>\n",
       "      <td>1.549684</td>\n",
       "      <td>0.721044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.187635</td>\n",
       "      <td>0.942719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>0.164334</td>\n",
       "      <td>0.955607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████████████████████████████████████████████████████████| 6.77k/6.77k [00:00<00:00, 3.31MB/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(classes, class_name),\n",
    "    args = args,\n",
    "    train_dataset = trainset,\n",
    "    eval_dataset = validset,\n",
    "    data_collator = lambda x: collator(x, transform),\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer = image_processor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d5f7a-8583-47df-a669-7f5c35464488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
